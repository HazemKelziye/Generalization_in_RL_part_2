extra_python_environs_for_driver: {}
extra_python_environs_for_worker: {}
num_gpus: 0
num_cpus_per_worker: 1
num_gpus_per_worker: 0
_fake_gpus: false
num_learner_workers: 0
num_gpus_per_learner_worker: 0
num_cpus_per_learner_worker: 1
local_gpu_idx: 0
custom_resources_per_worker: {}
placement_strategy: PACK
eager_tracing: true
eager_max_retraces: 20
tf_session_args:
  intra_op_parallelism_threads: 2
  inter_op_parallelism_threads: 2
  gpu_options:
    allow_growth: true
  log_device_placement: false
  device_count:
    CPU: 1
  allow_soft_placement: true
local_tf_session_args:
  intra_op_parallelism_threads: 8
  inter_op_parallelism_threads: 8
torch_compile_learner: false
torch_compile_learner_what_to_compile: !!python/object/apply:ray.rllib.core.learner.learner.TorchCompileWhatToCompile
- forward_train
torch_compile_learner_dynamo_backend: inductor
torch_compile_learner_dynamo_mode: null
torch_compile_worker: false
torch_compile_worker_dynamo_backend: onnxrt
torch_compile_worker_dynamo_mode: null
env: Pendulum-v1
env_config: {}
observation_space: null
action_space: null
env_task_fn: null
render_env: false
clip_rewards: null
normalize_actions: true
clip_actions: false
disable_env_checking: false
auto_wrap_old_gym_envs: true
action_mask_key: action_mask
_is_atari: null
env_runner_cls: null
num_envs_per_worker: 1
enable_connectors: true
rollout_fragment_length: auto
batch_mode: truncate_episodes
validate_workers_after_construction: true
compress_observations: false
sampler_perf_stats_ema_coef: null
sample_async: -1
remote_worker_envs: false
remote_env_batch_wait_ms: 0
enable_tf1_exec_eagerly: false
sample_collector: !!python/name:ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector ''
preprocessor_pref: deepmind
observation_filter: NoFilter
update_worker_filter_stats: true
use_worker_filter_stats: true
gamma: 0.95
lr: 0.01
grad_clip: null
grad_clip_by: global_norm
train_batch_size: 32
model:
  _disable_preprocessor_api: false
  _disable_action_flattening: false
  fcnet_hiddens:
  - 256
  - 256
  fcnet_activation: tanh
  conv_filters: null
  conv_activation: relu
  post_fcnet_hiddens: []
  post_fcnet_activation: relu
  free_log_std: false
  no_final_linear: false
  vf_share_layers: true
  use_lstm: false
  max_seq_len: 20
  lstm_cell_size: 256
  lstm_use_prev_action: false
  lstm_use_prev_reward: false
  _time_major: false
  use_attention: false
  attention_num_transformer_units: 1
  attention_dim: 64
  attention_num_heads: 1
  attention_head_dim: 32
  attention_memory_inference: 50
  attention_memory_training: 50
  attention_position_wise_mlp_dim: 32
  attention_init_gru_gate_bias: 2.0
  attention_use_n_prev_actions: 0
  attention_use_n_prev_rewards: 0
  framestack: true
  dim: 84
  grayscale: false
  zero_mean: true
  custom_model: null
  custom_model_config: {}
  custom_action_dist: null
  custom_preprocessor: null
  encoder_latent_dim: null
  always_check_shapes: false
  lstm_use_prev_action_reward: -1
  _use_default_native_models: -1
optimizer: {}
max_requests_in_flight_per_sampler_worker: 2
_learner_class: null
explore: true
exploration_config:
  type: StochasticSampling
algorithm_config_overrides_per_module: {}
policy_map_capacity: 100
policy_mapping_fn: !!python/name:ray.rllib.algorithms.algorithm_config.DEFAULT_POLICY_MAPPING_FN ''
policies_to_train: null
policy_states_are_swappable: false
observation_fn: null
count_steps_by: env_steps
input_config: {}
actions_in_input_normalized: false
postprocess_inputs: false
shuffle_buffer_size: 0
output: null
output_config: {}
output_compress_columns:
- obs
- new_obs
output_max_file_size: 67108864
offline_sampling: false
evaluation_interval: null
evaluation_duration: 10
evaluation_duration_unit: episodes
evaluation_sample_timeout_s: 180.0
evaluation_parallel_to_training: false
evaluation_config: null
off_policy_estimation_methods: {}
ope_split_batch_by_episode: true
evaluation_num_workers: 0
always_attach_evaluation_results: false
enable_async_evaluation: false
in_evaluation: false
sync_filters_on_rollout_workers_timeout_s: 60.0
keep_per_episode_custom_metrics: false
metrics_episode_collection_timeout_s: 60.0
metrics_num_episodes_for_smoothing: 100
min_time_s_per_iteration: 1
min_train_timesteps_per_iteration: 0
min_sample_timesteps_per_iteration: 100
export_native_model_files: false
checkpoint_trainable_policies_only: false
logger_creator: null
logger_config: null
log_level: WARN
log_sys_usage: true
fake_sampler: false
seed: null
ignore_worker_failures: false
recreate_failed_workers: false
max_num_worker_restarts: 1000
delay_between_worker_restarts_s: 60.0
restart_failed_sub_environments: false
num_consecutive_worker_failures_tolerance: 100
worker_health_probe_timeout_s: 60
worker_restore_timeout_s: 1800
_rl_module_spec: null
_AlgorithmConfig__prior_exploration_config: null
_enable_new_api_stack: false
_tf_policy_handles_more_than_one_loss: false
_disable_preprocessor_api: false
_disable_action_flattening: false
_disable_execution_plan_api: true
_disable_initialize_loss_from_dummy_batch: false
simple_optimizer: -1
policy_map_cache: -1
worker_cls: -1
synchronize_filters: -1
replay_sequence_length: null
twin_q: true
q_model_config:
  fcnet_hiddens:
  - 256
  - 256
  fcnet_activation: relu
  post_fcnet_hiddens: []
  post_fcnet_activation: null
  custom_model: null
  custom_model_config: {}
policy_model_config:
  fcnet_hiddens:
  - 256
  - 256
  fcnet_activation: relu
  post_fcnet_hiddens: []
  post_fcnet_activation: null
  custom_model: null
  custom_model_config: {}
tau: 0.005
initial_alpha: 1.0
target_entropy: auto
n_step: 1
replay_buffer_config:
  _enable_replay_buffer_api: true
  type: MultiAgentPrioritizedReplayBuffer
  capacity: 1000000
  prioritized_replay: false
  prioritized_replay_alpha: 0.6
  prioritized_replay_beta: 0.4
  prioritized_replay_eps: 1.0e-06
  worker_side_prioritization: false
store_buffer_in_checkpoints: false
training_intensity: null
optimization:
  actor_learning_rate: 0.0003
  critic_learning_rate: 0.0003
  entropy_learning_rate: 0.0003
target_network_update_freq: 0
num_steps_sampled_before_learning_starts: 1500
_deterministic_loss: false
_use_beta_distribution: false
use_state_preprocessor: -1
worker_side_prioritization: -1
input: sampler
policies:
  default_policy: !!python/tuple
  - null
  - null
  - null
  - null
callbacks: !!python/name:ray.rllib.algorithms.callbacks.DefaultCallbacks ''
create_env_on_driver: false
custom_eval_function: null
framework: torch
num_cpus_for_driver: 1
num_workers: 1
